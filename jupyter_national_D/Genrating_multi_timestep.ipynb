{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:15:33.197709Z",
     "iopub.status.busy": "2022-09-20T01:15:33.197374Z",
     "iopub.status.idle": "2022-09-20T01:15:34.804302Z",
     "shell.execute_reply": "2022-09-20T01:15:34.803926Z",
     "shell.execute_reply.started": "2022-09-20T01:15:33.197637Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout,Dense,LSTM,GRU,Conv2D,Activation,BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OrdinalEncoder,StandardScaler\n",
    "import joblib\n",
    "import os\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.dates as mdates\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "from tensorflow.keras import Model\n",
    "import itertools\n",
    "from functools import partial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:15:39.201648Z",
     "iopub.status.busy": "2022-09-20T01:15:39.201209Z",
     "iopub.status.idle": "2022-09-20T01:15:39.209510Z",
     "shell.execute_reply": "2022-09-20T01:15:39.208920Z",
     "shell.execute_reply.started": "2022-09-20T01:15:39.201622Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 指定训练使用的GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 设置log输出信息，2是打印error\n",
    "# 设置全局控制的变量\n",
    "freq = 'D'\n",
    "region = 'NA'\n",
    "\n",
    "if freq == 'D':\n",
    "    list_col = ['NO2+d','NO2-d','NO2-2d','NO2-3d']\n",
    "    file_name = 'demo_data.csv'\n",
    "\n",
    "file_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:12.155068Z",
     "iopub.status.busy": "2022-09-20T01:16:12.154764Z",
     "iopub.status.idle": "2022-09-20T01:16:21.627692Z",
     "shell.execute_reply": "2022-09-20T01:16:21.626943Z",
     "shell.execute_reply.started": "2022-09-20T01:16:12.155043Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:21.628766Z",
     "iopub.status.busy": "2022-09-20T01:16:21.628639Z",
     "iopub.status.idle": "2022-09-20T01:16:22.052710Z",
     "shell.execute_reply": "2022-09-20T01:16:22.052002Z",
     "shell.execute_reply.started": "2022-09-20T01:16:21.628754Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train_set = data[data.year < 2020].reset_index(drop=True)\n",
    "test_set = data[data.year == 2020].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:22.053573Z",
     "iopub.status.busy": "2022-09-20T01:16:22.053385Z",
     "iopub.status.idle": "2022-09-20T01:16:22.057279Z",
     "shell.execute_reply": "2022-09-20T01:16:22.056538Z",
     "shell.execute_reply.started": "2022-09-20T01:16:22.053560Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = data.columns.drop(['city',list_col[0],'station','geometry','index'])\n",
    "#cols = data.columns.drop(['city',list_col[0],'station','index'])\n",
    "index_std_NO2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 需要从数据集中取出进行标准化特征列\n",
    "cols = ['SCTC', 'PM2.5', 'Dew Point Temp', 'O3','Sea Level Pres', 'AQI', 'dist', 'PM10','NO2-d','NO2-2d','NO2-3d'\n",
    "    , 'Wind Dire', 'Elev', 'CO','Wind Speed', 'NO2', 'SO2', 'Air Temp', 'year', 'month', 'day', 'hour','day_of_week'\n",
    "    , 'week', 'day_of_year', 'season','is_weekend', 'lon_m', 'lon', 'lat_m', 'lat' ,'dis_pop','people_density'\n",
    "    ,'road_density','PBLH','HLML','QSH']\n",
    "index_std_NO2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:22.058490Z",
     "iopub.status.busy": "2022-09-20T01:16:22.058364Z",
     "iopub.status.idle": "2022-09-20T01:16:22.074774Z",
     "shell.execute_reply": "2022-09-20T01:16:22.074169Z",
     "shell.execute_reply.started": "2022-09-20T01:16:22.058479Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 函数：将特征标准化\n",
    "def data_std(train_set,test_set,num_col=38):\n",
    "    global std,index_std_NO2\n",
    "    std = StandardScaler()\n",
    "    col =train_set[cols].columns\n",
    "    train_ = std.fit_transform(train_set[cols])\n",
    "    train_ = pd.DataFrame(train_,columns=col)\n",
    "    test_ = std.transform(test_set[cols])\n",
    "    test_ = pd.DataFrame(test_,columns=col)\n",
    "    index_std_NO2 = train_.columns.tolist().index('NO2')\n",
    "\n",
    "   # list_col = ['NO2', 'season', 'SCTC', 'NO2-d','O3', 'period_of_day', 'hour', 'lon_m', 'lon', 'dist','year', 'week','lat_m'\n",
    "        #,'Dew Point Temp','day_of_year','CO','lat','Air Temp', 'NO2-3d','NO2-2d','PM2.5','day','month','PM10','Sea Level Pres'\n",
    "        #,'day_of_week','SO2','AQI', 'Wind Speed','Wind Dire','Elev','is_weekend']\n",
    "    #train_ = train_.loc[:,list_col[:num_col]]\n",
    "    #test_ = test_.loc[:,list_col[:num_col]]\n",
    "    # 添加数据关键信息，站点名和index等\n",
    "    train_['index'] = train_set['index']\n",
    "    train_['station']=train_set['station']\n",
    "    test_['index'] = test_set['index']\n",
    "    test_['station'] = test_set['station']\n",
    "    return train_,test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:22.075801Z",
     "iopub.status.busy": "2022-09-20T01:16:22.075537Z",
     "iopub.status.idle": "2022-09-20T01:16:22.096821Z",
     "shell.execute_reply": "2022-09-20T01:16:22.096222Z",
     "shell.execute_reply.started": "2022-09-20T01:16:22.075781Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 函数：填补时间序列\n",
    "def impute_time(group,start,end,sta,lists,locks):\n",
    "\n",
    "    group = group.set_index('index')\n",
    "    group.index = pd.to_datetime(group.index)\n",
    "    new_index = pd.date_range(start,end,freq=freq)[:-1]\n",
    "    group = group.reindex(new_index)\n",
    "    group['station'] = sta\n",
    "    \n",
    "    locks.acquire()\n",
    "    lists.append(group)\n",
    "    locks.release()\n",
    "\n",
    "def impute_time_multi(df,start,end,targets=impute_time):\n",
    "    lists = mp.Manager().list()\n",
    "    locks = mp.Manager().Lock()\n",
    "    pool1 = mp.Pool()\n",
    "    for sta,group in df.groupby('station'):\n",
    "        pool1.apply_async(targets,args=(group,start,end,sta,lists,locks))\n",
    "    pool1.close()\n",
    "    pool1.join()\n",
    "    data_1 = pd.concat(lists)\n",
    "    data_1 = data_1.reset_index()\n",
    "    return data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:22.097967Z",
     "iopub.status.busy": "2022-09-20T01:16:22.097703Z",
     "iopub.status.idle": "2022-09-20T01:16:22.113578Z",
     "shell.execute_reply": "2022-09-20T01:16:22.113004Z",
     "shell.execute_reply.started": "2022-09-20T01:16:22.097947Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 函数：将数据转换为三维数据,并提取标签\n",
    "def data_transform_s(step,group):\n",
    "    \n",
    "    xx = []\n",
    "    yy = []\n",
    "    index_NO2 = group.columns.tolist().index('NO2')\n",
    "    index_time = group.columns.tolist().index('index')\n",
    "    index_station = group.columns.tolist().index('station')\n",
    "    for i in range(step,group.shape[0]):\n",
    "        # 筛选还有缺失值的展开步长不足的数据块\n",
    "        if group.iloc[i-step:i,:].isnull().sum().values.sum() == 0 and group.iloc[i-step:i,:].shape[0] == step and np.isnan(group.iloc[i,index_NO2]) == False:\n",
    "            #print(group.iloc[i,index_NO2])    \n",
    "            xx.append(np.array(group.iloc[i-step:i,:].drop(columns=['index','station'])))\n",
    "            yy.append(np.array(group.iloc[i,[index_NO2,index_time,index_station]]))\n",
    "    \n",
    "    xx = np.array(xx).astype('float32')\n",
    "    yy = np.array(yy)\n",
    "    return xx,yy\n",
    "\n",
    "def concat_array(df):\n",
    "    xxxx = np.array(df)\n",
    "    x_sum = []\n",
    "    y_sum = []\n",
    "    for i in range(0,xxxx.shape[0]):\n",
    "        x_np = xxxx[i,0]\n",
    "        y_np = xxxx[i,1]\n",
    "        if x_np.shape[0] != 0:\n",
    "            x_sum.append(x_np)\n",
    "            y_sum.append(y_np)\n",
    "    \n",
    "    x_sum = np.concatenate(x_sum)\n",
    "    y_sum = np.concatenate(y_sum)\n",
    "    #print(x_sum.shape,y_sum.shape)\n",
    "    return x_sum,y_sum\n",
    "\n",
    "            \n",
    "def data_transform_multi(df,step):\n",
    "    #x = mp.Manager().list()\n",
    "    #y = mp.Manager().list()\n",
    "    #locks = mp.Manager().Lock()\n",
    "    \n",
    "    \n",
    "    df_list = []\n",
    "    for i,j in df.groupby(['station']):\n",
    "        df_list.append(j)\n",
    "    \n",
    "    #定义偏函数传入step\n",
    "    pool = mp.Pool()\n",
    "    adjust = partial(data_transform_s,step)\n",
    "    res = pool.map(adjust,df_list[:500]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    pool = mp.Pool()\n",
    "    adjust = partial(data_transform_s,step)\n",
    "    res_1 = pool.map(adjust,df_list[500:1000]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    pool = mp.Pool()\n",
    "    adjust = partial(data_transform_s,step)\n",
    "    res_2 = pool.map(adjust,df_list[1000:1400]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    pool = mp.Pool()\n",
    "    adjust = partial(data_transform_s,step)\n",
    "    res_3 = pool.map(adjust,df_list[1400:]) \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    res = res + res_1 + res_2 + res_3 \n",
    "    del df_list,res_1,res_2,res_3,pool\n",
    "    gc.collect()\n",
    "    \n",
    "    x,y = concat_array(res)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:22.114602Z",
     "iopub.status.busy": "2022-09-20T01:16:22.114332Z",
     "iopub.status.idle": "2022-09-20T01:16:22.135378Z",
     "shell.execute_reply": "2022-09-20T01:16:22.134861Z",
     "shell.execute_reply.started": "2022-09-20T01:16:22.114583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 函数：数据维度转换，打乱和reshape\n",
    "def train_test_to_3D(train_,test_,step):\n",
    "    # 填补时间序列\n",
    "    train = impute_time_multi(train_,'2014-05-13','2020-01-01')\n",
    "    test = impute_time_multi(test_,'2020-01-01','2021-01-01')\n",
    "    # 找到NO2所在的列数\n",
    "    \n",
    "    # 划分训练集和测试集的特征、标签\n",
    "\n",
    "    xtrain,ytrain_ = data_transform_multi(train,step)\n",
    "    xtest,ytest_ = data_transform_multi(test,step)\n",
    "\n",
    "    del train,test\n",
    "    gc.collect()\n",
    "    \n",
    "    # 对训练集进行打乱\n",
    "    np.random.seed(7)\n",
    "    np.random.shuffle(xtrain)\n",
    "    np.random.seed(7)\n",
    "    np.random.shuffle(ytrain_)\n",
    "    # 将训练集由list转换位array\n",
    "    xtrain,ytrain_ = np.array(xtrain),np.array(ytrain_)\n",
    "    # 将训练集转化为可输入模型的格式\n",
    "    xtrain  = np.reshape(xtrain,(xtrain.shape[0],step,train_.shape[1]-2))\n",
    "    #ytrian_ = ytrain_.reshape(len(ytrain_),3)\n",
    "    # ytrian_:带时间索引和站点名称的数据\n",
    "    ytrain = np.array(list(ytrain_[:,0]))\n",
    "\n",
    "    # 测试集数据转换\n",
    "    xtest,ytest_ = np.array(xtest),np.array(ytest_)\n",
    "    xtest = np.reshape(xtest,(xtest.shape[0],step,train_.shape[1]-2))\n",
    "    ytest = np.array(list(ytest_[:,0]))\n",
    "\n",
    "    return xtrain,ytrain,xtest,ytest,ytrain_,ytest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:22.136319Z",
     "iopub.status.busy": "2022-09-20T01:16:22.136117Z",
     "iopub.status.idle": "2022-09-20T01:16:23.268628Z",
     "shell.execute_reply": "2022-09-20T01:16:23.267946Z",
     "shell.execute_reply.started": "2022-09-20T01:16:22.136299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对训练集和测试集进行标准化，并分别划分转化特征和标签.\n",
    "train_,test_ = data_std(train_set,test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:24.580441Z",
     "iopub.status.busy": "2022-09-20T01:16:24.580143Z",
     "iopub.status.idle": "2022-09-20T01:16:24.715029Z",
     "shell.execute_reply": "2022-09-20T01:16:24.714461Z",
     "shell.execute_reply.started": "2022-09-20T01:16:24.580416Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data, train_set, test_set\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:16:25.633945Z",
     "iopub.status.busy": "2022-09-20T01:16:25.633647Z",
     "iopub.status.idle": "2022-09-20T01:19:58.640314Z",
     "shell.execute_reply": "2022-09-20T01:19:58.639715Z",
     "shell.execute_reply.started": "2022-09-20T01:16:25.633920Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdtls/anaconda3/envs/tf2.1/lib/python3.7/site-packages/ipykernel_launcher.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    xtrain,ytrain,xtest,ytest,ytrain_,ytest_ = train_test_to_3D(train_,test_,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:19:58.641337Z",
     "iopub.status.busy": "2022-09-20T01:19:58.641211Z",
     "iopub.status.idle": "2022-09-20T01:19:58.645295Z",
     "shell.execute_reply": "2022-09-20T01:19:58.644865Z",
     "shell.execute_reply.started": "2022-09-20T01:19:58.641324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "print(index_std_NO2,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:24:51.628728Z",
     "iopub.status.busy": "2022-09-20T01:24:51.628425Z",
     "iopub.status.idle": "2022-09-20T01:24:51.633249Z",
     "shell.execute_reply": "2022-09-20T01:24:51.632699Z",
     "shell.execute_reply.started": "2022-09-20T01:24:51.628703Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:24:53.194820Z",
     "iopub.status.busy": "2022-09-20T01:24:53.194337Z",
     "iopub.status.idle": "2022-09-20T01:25:09.244888Z",
     "shell.execute_reply": "2022-09-20T01:25:09.244114Z",
     "shell.execute_reply.started": "2022-09-20T01:24:53.194794Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save data and laebls\n",
    "np.savez(file_path + 'numpy/NA_d_array_14.npz',a=xtrain,b=ytrain,c=xtest,d=ytest,e=ytrain_,f=ytest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:25:09.245846Z",
     "iopub.status.busy": "2022-09-20T01:25:09.245721Z",
     "iopub.status.idle": "2022-09-20T01:25:09.248919Z",
     "shell.execute_reply": "2022-09-20T01:25:09.248593Z",
     "shell.execute_reply.started": "2022-09-20T01:25:09.245833Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:25:09.249479Z",
     "iopub.status.busy": "2022-09-20T01:25:09.249361Z",
     "iopub.status.idle": "2022-09-20T01:25:09.290572Z",
     "shell.execute_reply": "2022-09-20T01:25:09.289905Z",
     "shell.execute_reply.started": "2022-09-20T01:25:09.249468Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save feature names\n",
    "pd.DataFrame(cols).to_csv(file_path + 'features_name/features_name_' +  freq, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-20T01:25:09.292201Z",
     "iopub.status.busy": "2022-09-20T01:25:09.291887Z",
     "iopub.status.idle": "2022-09-20T01:25:09.316759Z",
     "shell.execute_reply": "2022-09-20T01:25:09.316162Z",
     "shell.execute_reply.started": "2022-09-20T01:25:09.292177Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save normalization parameters\n",
    "norma_param = pd.DataFrame([std.var_,std.mean_],index=['std', 'mean']).T\n",
    "norma_param.to_csv(file_path + 'normalization_params/normalization_params_' +  freq + '_no2-idx-0', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tf2.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6da420b591d19b96bb228d378af550d31e7605dfc652be5a414a014d4870d22e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
